{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65baa88b-0470-471b-9042-496a0daac238",
      "metadata": {
        "id": "65baa88b-0470-471b-9042-496a0daac238"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import autograd\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image, ImageFilter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8063bd22-23f9-4f31-9a77-69f68b9eafc2",
      "metadata": {
        "id": "8063bd22-23f9-4f31-9a77-69f68b9eafc2"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        self.image_dir = 'images128x128'  # Directory path for images\n",
        "        self.image_paths = sorted([os.path.join(self.image_dir, f) for f in os.listdir(self.image_dir) if f.endswith('.png')])\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=4, tileGridSize=(8,8))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Convert the PIL image to a NumPy array\n",
        "        img_np = np.array(img)\n",
        "\n",
        "        # Apply CLAHE using OpenCV\n",
        "        img_clahe = self.clahe.apply(img_np)\n",
        "\n",
        "        # Convert back to PIL image (if your transformations expect PIL images)\n",
        "        img = Image.fromarray(img_clahe)\n",
        "\n",
        "        # Extract label from filename\n",
        "        label = int(os.path.basename(img_path).split('_')[1].split('.')[0])\n",
        "\n",
        "        # Apply any additional transformations if specified\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c83bbcb-ffa6-4556-8206-fafedfbfa9d6",
      "metadata": {
        "id": "4c83bbcb-ffa6-4556-8206-fafedfbfa9d6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread('images128x128/image1_0.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Apply Histogram Equalization\n",
        "equalized_image = cv2.equalizeHist(image)\n",
        "\n",
        "# Apply CLAHE\n",
        "clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))\n",
        "clahe_image = clahe.apply(image)\n",
        "\n",
        "# Display CLAHE result\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"CLAHE Image\")\n",
        "plt.imshow(clahe_image, cmap='gray')\n",
        "plt.show()\n",
        "# Apply Sobel operator\n",
        "sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)  # Horizontal edges\n",
        "sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)  # Vertical edges\n",
        "sobel_combined = cv2.magnitude(sobelx, sobely)\n",
        "\n",
        "# Display Sobel result\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Sobel X\")\n",
        "plt.imshow(sobelx, cmap='gray')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Sobel Y\")\n",
        "plt.imshow(sobely, cmap='gray')\n",
        "plt.show()\n",
        "# Apply simple thresholding\n",
        "ret, thresholded_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Create a sharpening kernel\n",
        "kernel = np.array([[0, -1, 0],\n",
        "                   [-1, 5,-1],\n",
        "                   [0, -1, 0]])\n",
        "\n",
        "# Apply the kernel to the image\n",
        "sharpened_image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "# Display sharpened image\n",
        "plt.title(\"Sharpened Image\")\n",
        "plt.imshow(sharpened_image, cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4eeb3ba-ec50-42ad-bb3e-30119fb90167",
      "metadata": {
        "id": "b4eeb3ba-ec50-42ad-bb3e-30119fb90167"
      },
      "outputs": [],
      "source": [
        "dataset = CustomImageDataset()\n",
        "dataset[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c5f52d-dada-47d5-9498-dcc1414df7c1",
      "metadata": {
        "id": "c7c5f52d-dada-47d5-9498-dcc1414df7c1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Load your images\n",
        "image_label_1 = cv2.imread('images128x128/image61_0.png', cv2.IMREAD_GRAYSCALE)\n",
        "image_label_2 = cv2.imread('images128x128/imagemass2119_1.png', cv2.IMREAD_GRAYSCALE)\n",
        "image_label_3 = cv2.imread('images128x128/imagepneumonia1111_2.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Create a 1x3 grid\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Display the images in each column\n",
        "axes[0].imshow(image_label_1, cmap='gray')\n",
        "axes[1].imshow(image_label_2, cmap='gray')\n",
        "axes[2].imshow(image_label_3, cmap='gray')\n",
        "\n",
        "# Remove axis ticks\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Add text labels below the images\n",
        "axes[0].text(0.5, -0.1, 'No Diagnosis', fontsize=12, ha='center', transform=axes[0].transAxes)\n",
        "axes[1].text(0.5, -0.1, 'Diagnosed: Mass', fontsize=12, ha='center', transform=axes[1].transAxes)\n",
        "axes[2].text(0.5, -0.1, 'Diagnosed: Pneumonia', fontsize=12, ha='center', transform=axes[2].transAxes)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e833193e-d21d-44bc-91c5-b360f004aa7d",
      "metadata": {
        "id": "e833193e-d21d-44bc-91c5-b360f004aa7d"
      },
      "outputs": [],
      "source": [
        "# Define transformations for 128x128 images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))  # Normalize for grayscale images\n",
        "])\n",
        "dataset = CustomImageDataset(transform=transform)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49da3a9-69a8-474d-8134-e14f2a206b59",
      "metadata": {
        "id": "c49da3a9-69a8-474d-8134-e14f2a206b59"
      },
      "outputs": [],
      "source": [
        "# Discriminator model for 128x128 images\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(10, 10)  # Assuming 10 classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(128 * 128 + 10, 1024),  # Input size is now 128*128 + 10\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the image tensor\n",
        "        c = self.label_emb(labels)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        out = self.model(x)\n",
        "        return out.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d426aa3-c1d3-4a3a-9adc-8938be710c44",
      "metadata": {
        "id": "0d426aa3-c1d3-4a3a-9adc-8938be710c44"
      },
      "outputs": [],
      "source": [
        "# Generator model for 128x128 images\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(10, 10)  # Assuming 10 classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100 + 10, 256),  # Assuming noise vector size is 100\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 128 * 128),  # Adjust output size to 128*128\n",
        "            nn.Tanh()  # Use Tanh to scale output to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        z = z.view(z.size(0), 100)  # Ensure z is of size (batch_size, 100)\n",
        "        c = self.label_emb(labels)\n",
        "        x = torch.cat([z, c], 1)  # Concatenate noise and label\n",
        "        out = self.model(x)\n",
        "        return out.view(z.size(0), 1, 128, 128)  # Reshape to (batch_size, 1, 128, 128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf41fa30-3abb-41e6-ad8b-2ce5baa9cd9e",
      "metadata": {
        "id": "cf41fa30-3abb-41e6-ad8b-2ce5baa9cd9e"
      },
      "outputs": [],
      "source": [
        "generator = Generator().cuda()\n",
        "discriminator = Discriminator().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0939869-e641-4d86-90b1-ffa68357124e",
      "metadata": {
        "id": "e0939869-e641-4d86-90b1-ffa68357124e"
      },
      "outputs": [],
      "source": [
        "#DONT CHANGE\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.000005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0106b8e-839f-498c-8ff6-40a46d9b5a26",
      "metadata": {
        "id": "e0106b8e-839f-498c-8ff6-40a46d9b5a26"
      },
      "outputs": [],
      "source": [
        "#criterion = nn.BCELoss()\n",
        "#d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.000005)\n",
        "#g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da39911-5619-4808-ae3c-ea43fe3487ab",
      "metadata": {
        "id": "0da39911-5619-4808-ae3c-ea43fe3487ab"
      },
      "outputs": [],
      "source": [
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
        "    g_optimizer.zero_grad()\n",
        "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
        "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    validity = discriminator(fake_images, fake_labels)\n",
        "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    return g_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c303876-d154-43ce-b47c-95704f35d548",
      "metadata": {
        "id": "9c303876-d154-43ce-b47c-95704f35d548"
      },
      "outputs": [],
      "source": [
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # Train with real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
        "\n",
        "    # Train with fake images\n",
        "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
        "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 3, batch_size))).cuda()\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    fake_validity = discriminator(fake_images, fake_labels)\n",
        "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
        "\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    return d_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb64955-6de8-4bf0-95c6-25eb6499b3f6",
      "metadata": {
        "scrolled": true,
        "id": "dcb64955-6de8-4bf0-95c6-25eb6499b3f6"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500\n",
        "x = 0\n",
        "for epoch in range(num_epochs):\n",
        "    x += 1\n",
        "    if(x%20 == 0):\n",
        "        torch.save(generator.state_dict(), f'generator{x}.pth')\n",
        "    print('Starting epoch {}...'.format(epoch))\n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "        real_images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        generator.train()\n",
        "        batch_size = real_images.size(0)\n",
        "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
        "                                          generator, d_optimizer, criterion,\n",
        "                                          real_images, labels)\n",
        "\n",
        "\n",
        "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
        "\n",
        "    generator.eval()\n",
        "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
        "    z = Variable(torch.randn(9, 100)).cuda()\n",
        "    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n",
        "    sample_images = generator(z, labels).data.cpu()\n",
        "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
        "    plt.imshow(grid)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11066fab-7773-473c-aa2b-e9391b39b09e",
      "metadata": {
        "id": "11066fab-7773-473c-aa2b-e9391b39b09e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a9f9b5-7b42-4cf7-8076-2b543bcfe708",
      "metadata": {
        "id": "60a9f9b5-7b42-4cf7-8076-2b543bcfe708"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}